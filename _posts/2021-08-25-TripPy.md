---
title:  "TripPy Paper review"
excerpt: "TripPy: A Triple Copy Strategy for Value Independent Neural Dialog State Tracking"

categories:
  - Paper review
tags:
  - Paper review
  - NLP
  - TOD
  - Dialog system
  - DST
  - Dialog State Tracking
last_modified_at: 2021-08-25T08:06:00-05:00
mathjax: true
---
[논문 링크](https://arxiv.org/pdf/2005.02877.pdf)

[이전 리뷰](https://momozzing.github.io/study/TOD,-DST/) -> TOD, DST 설명 
## **introduction**

Task-Orientation Dialog system의 구성 요소중에 DST 단계가 있다. DST를 위한 논문.

현재 기준 Multi-domain DST 3위 모델 

DST는 대화중에 계속 달라지는 slot의 값을 Tracking해서  대화 끝에 나오는 최종적인 해당 slot를 추출하는 Task이다. 

본 논문에서는 DST를 위한 다양한 Copy mechanism을 사용하는 새로운 방법론을 제시.

Slot은 3가지의 copy mechanism중 하나로 채워진다.

1. Span Prediction -> 현재 대화에서 바로 추출할 수 있음.
2. System imform memory -> System의 inform을 Tracking하는 system imform memory에서 값을 복사해서 사용할 수 있음.
3. DS Memory -> Domain 내에 Domain 간의 coreference와 value sharing problems을 해결하기 위해 이미 채워진 다른 Slot값을 복사해서 현재 Slot에 채울 수 있다. 
(이미 존재하는 slot을 사용해서 새로운 slot을 추론하는 방법)

 
이 접근 방식은 Span-based 방식의 slot filling의 장점과 메모리에서 copy하는 방법을 사용 

이 방법론은 System imform memory와  DS Memory(이전에 본 슬롯)에 대한 두 개의 메모리를 즉석에서 생성하고 유지해서 위의 2, 3번의 값을 채울 수 있다. 


이전 방식들은 slot-value 값을 고정시켜서 해당하는 slot을 추출하는 방식으로 진행을 해왔지만 이런 방식은 고정된 데이터안에서만 높은 성능을 낼 수 있다. 

이를 더 복잡한 데이터 안에 넣게 되면 다양한 한계점이 드러난다. 

1. 작업에 대한 완전한 slot-value값을 고정시키기 어렵다. 
2. 고정시킨 slot-value값 밖에 있는 다른 slot-value들은 추출하지 못한다. 
3. 이러한 방법론은 고정시킨 slot-value값의 데이터 크기와 성능이 선형적으로 확장된다.(비례한다)

실제 대화는 무수히 많은 어휘들과 slot-value값을 가질 수 있기 때문이다. 



## **Model Architecture**
![image](https://user-images.githubusercontent.com/60643542/130750184-2dc312ea-36b8-4c4d-8867-c0c0ebbfb979.png)

### 요약
BERT의 인코더 방식을 사용하고 output에 ${\theta}_{span}$ = Span prediction layer(Gate), ${\theta}_{gate}$ = Slot gate, ${\theta}_{bgate}$ = binary slot gate,
${\theta}_{refer}$ = Coreference gate 를 달아 각각의 Copy mechanism을 수행 하며 
$a_t^{inform}$ = sysytem inform memory와 $a_t^{ds}$ = DS(domain slot) memory을 현재 상황정보를 보존할수 있는 Auxiliary input으로 사용한다.
### **Context Encoder**

BERT의 인코더 방식을 사용하였다.

$$R_t = BERT([CLS])\oplus U_t \oplus [SEP] \oplus M_t \oplus [SEP] \oplus H_t \oplus[SEP])$$

$H_t = (U_{t-1}, M_{t-1}),...,(U_1,M_1)$ -> dialog hustory, $t$ = turn

$U$ = User utterance, $M$ = System utterance

$R_t = [r_t^{CLS},r_t^1,...,r_t^{seqmax}]$ -> BERT인코딩결과 

$r_t^{CLS}$ = 한 턴의 모든 context의 representation을 담고있음 

$r_t^1,...,r_t^{seqmax}$ 각 sequence의 input token에 대한 representation이 담겨있음 

이 representation 결과를 이용해서 slot에 대한 classification 할 수 있음. 

### **Slot Gates**


$C$ = Slot Class

$C = {\{none, doncate, span, inform, refer}\}$

- $none$ = 슬롯이 없는것
- $dontcare$ = slot에 대한 모든값 허용  
- $span$ = span prediction값으로 예측
- $inform$ = system inform memory값 사용
- $refer$ = DS memory값 사용

Slot gates 수식
$$\mathbf{softmax}(W_s^{gate}\cdot r_t^{CLS} + b_s^{gate}) \in \mathbb{R}^5 $$ 

Binary Slot gate 수식

$$\mathbf{softmax}(W_s^{bgate}\cdot r_t^{CLS} + b_s^{bgate}) \in \mathbb{R}^4 $$ 

### **System Inform Memory for Value Prediction**

system inform memery = system에서 원하는 slot이 나왔을경우 이를 복사해서 사용 

$I_t = \{I_t^1,...,I_t^N\}$

slot gate 가 inform으로 prediction시에 $DS_t^s = I_t^s$값으로 copy후 사용 

### **DS Memory for Coreference Resolution**

DS Memory = 대화중에 도메인의 변경이 있을 시 다른 슬롯에 할당된 값을 copy후 사용

copy하는 다른 slot에 대한 확률분포는 

$$p_{t,s}^{refer}(r_t^{CLS}) = \mathbf{softmax}(W_{refer}^s\cdot r_t^{CLS} + (b_{refer}^s) \in \mathbb{R}^{N+1}$$

copy값이 중복이 되거나 copy값이 없을 시 $none$ 반환 


### **Auxiliary Features**

최근 연구 방향은 상황 정보를 보존하기 위해 보조 입력을 사용한다

system imform memory, DS memory 기반으로 보조 입력을 생성한다. 

$a_t^{inform}$은 system imform memory를 언제 copy해서 사용했는지 

$a_t^{ds}$은 DS memory를 사용했는데 slot이 이미 채워져 있는것을 나타낸다.

$$p_{t,s}^{gate}(\hat{r}_t^{CLS}),p_{t,s}^{bgate}(\hat{r}_t^{CLS}), p_{t,s}^{refer}(\hat{r}_t^{CLS})$$
위의 친구들의 $(\hat{r}_t^{CLS})$ 부분에 추가로 보조입력들을 추가로 더해 학습하게 된다. (BERT의 출력에 추가해서 학습한다.)

$$\hat{r}_t^{CLS} = r_t^{CLS} \oplus a_t^{inform} \oplus a_t^{ds}$$

### **Partial Masking**

dialog history 부분에 [UNK]토큰을 넣어서 부분적으로 마스킹을 씌우는 방법이다.

dialog history부분에 [UNK]토큰을 넣게 되면 과거의 context 정보에 좀 더 집중을 하게 만들 수 있어 $r_t^{CLS}$가 좀 더 강력한 representation을 가질 수 있다

### **Dialog State Update**

턴을 기준으로 DST를 진행하는데 slot이 detection이 되지 않으면 매 턴마다 슬롯을 업데이트 한다. slot값이 $none$으로 예측되면 슬롯이 업데이트 되지 않는다. 

## **Experiment**

### **Data**
데이터셋은 MultiWOZ 2.1, WOZ 2.0, sim-M. sim-R에 대해서 진행한다. 

MultiWOZ2.1은 10000개 이상의 multi-domain 발화로 구성 되어 있으며 30개의 domain-slot쌍과 5개의 도메인(기차, 레스토랑, 호텔, 택시, 명소)로 구성되어 있다. 
다른 데이터셋들은 단일 도메인 데이터셋들이며 MultiWOZ2.1 보다 훨씬 작은데이터다.

### **Evaluation**
joint goal accuracy (JGA)로 평가 진행

### **Training**

Joint Loss를 구하게 되는데 Multi domain은 아래와 같이 진행한다. 

loss = CE, optimizer = Adam, lr = $2e^{-5}$, warm-up = 10%, Dropout = 0.3

$$\mathcal{L} = 0.8 \cdot \mathcal{L}_{gate} + 0.1 \cdot \mathcal{L}_{span} + 0.1 \cdot \mathcal{L}_{refer}$$

single domain 은 domain 간 coreference가 없음으로 refer는 제거하고 진행 

$$\mathcal{L} = 0.8 \cdot \mathcal{L}_{gate} + 0.2 \cdot \mathcal{L}_{span}$$

### **result**
MultiWOZ 2.1

![image](https://user-images.githubusercontent.com/60643542/130822827-03fb0897-a0c4-4ff4-a327-ad7d9bb7b9e5.png)

WOZ 2.0

![image](https://user-images.githubusercontent.com/60643542/130822922-83e60c56-3251-4bbc-869d-75b66b897b32.png)

sim-M. sim-R

![image](https://user-images.githubusercontent.com/60643542/130823470-b4c95b1f-fcf9-482f-a48f-e8febf32bc77.png)

실험 비교

![image](https://user-images.githubusercontent.com/60643542/130826813-2118bfce-0386-40d6-b843-b427923cc2cb.png)

## **Conclusion**

본 논문에서는 DST를 위한 다양한 Copy mechanism을 사용하는 새로운 방법론을 제시.

Slot은 3가지의 copy mechanism중 하나로 채워진다.

1. Span Prediction -> 현재 대화에서 바로 추출할 수 있음.
2. System imform memory -> System의 inform을 Tracking하는 system imform memory에서 값을 복사해서 사용할 수 있음.
3. DS Memory -> Domain 내에 Domain 간의 coreference와 value sharing problems을 해결하기 위해 이미 채워진 다른 Slot값을 복사해서 현재 Slot에 채울 수 있다. 
(이미 존재하는 slot을 사용해서 새로운 slot을 추론하는 방법)

 
이 접근 방식은 Span-based 방식의 slot filling의 장점과 메모리에서 copy하는 방법을 사용 

본 논문에서 제안한 방법론들 모두 좋은 성능향상을 보였다. 

현재 기준 multi-domain DST 3위 모델