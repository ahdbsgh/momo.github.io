---
title:  "Slot-Gate Paper review"
excerpt: "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction"

categories:
  - Paper review
tags:
  - Paper review
  - NLP
  - TOD
  - Dialog system
  - Slot-filling
  - intent detection
last_modified_at: 2021-08-24T08:06:00-05:00
---
[논문 링크](https://aclanthology.org/N18-2118.pdf)

[이전 리뷰](https://momozzing.github.io/paper%20review/A-survey-of-joint-learning-models-to-intent-detection-and-slot-filling/) -> joint lerning for intent detection and slot filling 참고 

## **introduction**

slot-gate 는 Slot Filling, intent detection을 동시에 학습하는 joint learning 을 위한 논문

본 논문은 slot과 intent는 강한 relation이 있음으로 global optimization을 통해 보다 나은 symantic frame 결과를 얻기 위해 slot과 intent의 attention vector사이의 관계를 학습하는데 초점을 두었다. 


## **Proposed Approach**
![image](https://user-images.githubusercontent.com/60643542/130598290-3ba46d4e-6c5f-4ed0-9b9a-72670fdba74f.png)

attention 기반의 BiLSTM로 이루어진 모델이다. 

$x = (x_1, ...,x_T)$ -> word sequence

$\overset{\rightarrow}{h_i}$ = forward hidden state 

$\overset{\leftarrow}{h_i}$ = backward hidden state
### slot filling 

final hidden state $h_i$ = [$\overset{\rightarrow}{h_i}$, $\overset{\leftarrow}{h_i}$]         -> concat 

input $x$는  outputd인 $y = (y_1^S,...,y_T^S)$ 와 매핑됨  


$$c_i^S = \sum_{j=1}^T a_{i,j}^S h_j$$

$c_i^S$ = slot context vector 

$a_i^S$ = attention weigth

${h_j}$ = hiddenstate 

$$a_{i,j}^S = {\exp(e_{i,j}) \over \sum_{k=1}^T\exp(e_{i,k})}$$

$$e_{i,k} = \sigma(W_{he}^Sh_k)$$

$\sigma$ = activate func

$W_{he}^S$ = weight matrix of feedfoward neural network

$$y_i^S = \mathbf{softmax}(W_{hy}^S(h_i + c_i^S)) $$

$y_i^S$ = slot label i-th word

$W_{hy}^S$ = weight matrix

## **Recursive neural networks**

- **Joint Semantic Utterance Classification and Slot filling with Recursive neural networks, Guo et al , IEEE 2014**

  joint learning을 다루기 위한 신경망 모델의 가장 초기 시도   
재귀 신경망 recursive neural networks (RecNN)을 사용 (RNN과는 아예 다름)  
RecNN은 트리(단어 벡터)에 해당하는 leaf 를 사용하여 발화의 구성 구문 분석 트리에서 작동한다.    
신경망은 트리의 각 노드에서 반복적으로 루트까지 적용되어 각 노드의 상태를 계산,   
각 노드에서 하위 노드의 상태는 노드의 구문 유형을 나타내는 가중치 벡터와 결합한다.   
개별 슬롯 레이블 분류기는 리프에서 루트까지의 경로를 따라 자신과 인접 단어 벡터 및 상태 벡터의 조합을 사용하여 각 리프에 적용된다. 
루트의 상태는 인텐트 분류기로 전달된다.   
슬롯과 인텐트 (및 도메인)에 대한 결합 된 loss은 backpropagation 됨.   
후처리(옵션)와 , Viterbi 디코딩 된 Markov 레이어가 슬롯에 적용된다.   

## **Recurrent neural networks**

- **A Hierarchical LSTM Model for Joint Tasks , Zhou et al, 2016**

  총 2계층의 NSTM layer을 사용 1층의 NSTM layer의 hidden state로 intent detection, 2층의 NSTM layer의 hidden state로 slot filling을 진행


- **Multi-Domain Joint Semantic Frame Parsing using Bi-directional RNN-LSTM, Hakkani-Tür et al, Interspeech 2016, Microsoft Redmond**

  intent detection에 사용하기 위해 발언을 캡슐화하기 위해 특별한 토큰을 추가 → EOS   
slot filling은 BiLSTM에 각각 연결된 hidden state를 사용하여 출력단에 soft max을 적용해 분류한다.










