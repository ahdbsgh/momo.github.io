---
title:  "효율적인 LLM 학습 전략"
excerpt: "효율적인 LLM 학습 전략"

categories:
  - Study
tags:
  - NLP
  - Large Language Model
  - PEFT
  - LLM Quantization

last_modified_at: 2023-12-03T08:06:00-05:00
mathjax: true
# classes: wide
toc: true
toc_sticky: true
---

# **introduction**

최근에는 LLM을 적은 GPU 내에서 Fine-tuning을 진행하려는 연구 분야가 있다.

대기업이나, 대규모 연구실 등, LLM을 연구/개발하는 사람들은

고객들에게 서비스를 하기 위해 모델을 효율적으로 압축(양자화)하고, 효율적으로 Fine-tuning 하는 방법을 사용한다.

이런 연구분야는 어떻게 발전되어 왔는지 살펴보자.


# **Method**

이 연구분야는 크게 두가지 갈래로 발전이 되어왔다. 

첫번째는, 모델을 압축(양자화) 하는 방법이다.

모델을 압축(양자화) 함으로써, 모델의 성능은 유지하고 GPU에 올라가는 모델의 메모리를 줄일 수 있다. 

두번째는, Fine-tuning을 진행할 때, 모델의 전체 파라미터를 학습시키는 것이 아니라, 

학습 가능한 적은양의 파라미터를 모델 내부에 삽입해 이 부분만 학습을 시킨다.

이렇게 하면, 적은양의 파라미터로 모델을 Fine-tuning 할 수 있고, 

모델의 성능은 유지하고 학습 시 소모되는 메모리 소모량을 감소 시킬 수 있다. 

## **Large Lanuage Model Quantization**

Quantization이란? 

모델의 파라미터를 Low bit로 표현하고 저장함으로써, 모델을 압축하는 방법.

기존의 언어모델의 파라미터는 Float32로 표현되었는데, 이를 FP16, FP8 과 같은 Low bit로 압축을 한다. 




## **Parameter-Efficient Fine-Tuning (PEFT)**



# **Conclusion**

LLM을 적은 GPU 내에서 Fine-tuning을 진행하기 위해서는 

PEFT, Quantization을 진행 해야한다. 

이전 연구들은 PEFT, Quantization 따로 진행하며 연구 되었지만,

최근에는 PEFT와 Quantization을 합쳐 좋은 성능을 내고있다.

앞으로 서비스 단계에서 고객에게 LLM을 온프레미스로 제공하고 데이터 구축 방법을 제공한다면

고객이 직접 간단하게 fine-tuning이 가능해질 것이다.

